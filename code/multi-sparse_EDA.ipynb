{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gc\n",
    "from utils import SPARSE_DATA_DIR, DATA_DIR\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_inputs_values = sp.sparse.load_npz(\n",
    "    f'{SPARSE_DATA_DIR}/train_multi_inputs_values.sparse.npz'\n",
    ")\n",
    "train_multi_targets_values = sp.sparse.load_npz(\n",
    "    f'{SPARSE_DATA_DIR}/train_multi_targets_values.sparse.npz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_inputs_idx = np.load(\n",
    "    f'{SPARSE_DATA_DIR}/train_multi_inputs_idxcol.npz'\n",
    ")\n",
    "train_multi_targets_idx = np.load(\n",
    "    f'{SPARSE_DATA_DIR}/train_multi_targets_idxcol.npz'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty long and very wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105942, 228942)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_multi_inputs_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105942, 23418)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_multi_targets_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the input data is very sparse. \n",
    "\n",
    "- What about output?\n",
    "\n",
    "\tFor CITE we know it's about 75% sparse, and this checks out:\n",
    "\t```\n",
    "\t2.3 GB   train_cite_inputs.h5\n",
    "\t711 MB  train_cite_inputs_values.sparse.npz\n",
    "\t```\n",
    "\tOutputs aren't really sparse at all\n",
    "\t```\n",
    "\t36 MB  train_cite_targets_values.sparse.npz\n",
    "\t37 MB   train_cite_targets.h5\n",
    "\t```\n",
    "\tFor multi, I read that inputs were about 95% sparse. Hmmm.\n",
    "\t```\n",
    "\t11 GB   train_multi_inputs.h5\n",
    "\t2.8 GB  train_multi_inputs_values.sparse.npz\n",
    "\t```\n",
    "\tRegardless, targets are about as sparse as CITE inputs, which makes sense.\n",
    "\t```\n",
    "\t3.0 GB   train_multi_targets.h5\n",
    "\t824 MB  train_multi_targets_values.sparse.npz\n",
    "\t```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do columns change in availability across input/output and technologies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow to even load just 1 row of multi, so commented out for another method\n",
    "# train_multi_target_cols = pd.read_hdf(f'{DATA_DIR}/train_multi_targets.h5', start=0, end=1)\n",
    "\n",
    "# very difficult to get this working...\n",
    "import hdf5plugin\n",
    "def extract_cols(filename):\n",
    "    f = h5py.File(f'{DATA_DIR}/{filename}')\n",
    "    key = list(f.keys())[0]\n",
    "    group = f[key]\n",
    "    axis = group['axis0']  # type: ignore\n",
    "    return pd.Series(axis[()])  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_targets_cols = extract_cols(\"train_multi_targets.h5\")\n",
    "train_multi_inputs_cols = extract_cols(\"train_multi_inputs.h5\")\n",
    "train_cite_inputs_cols = extract_cols(\"train_cite_inputs.h5\")\n",
    "train_cite_targets_cols = extract_cols(\"train_cite_targets.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       b'CD86'\n",
       "1      b'CD274'\n",
       "2      b'CD270'\n",
       "3      b'CD155'\n",
       "4      b'CD112'\n",
       "         ...   \n",
       "135    b'HLA-E'\n",
       "136     b'CD82'\n",
       "137    b'CD101'\n",
       "138     b'CD88'\n",
       "139    b'CD224'\n",
       "Length: 140, dtype: bytes88"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cite_targets_cols # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            b'ENSG00000121410_A1BG'\n",
       "1        b'ENSG00000268895_A1BG-AS1'\n",
       "2             b'ENSG00000175899_A2M'\n",
       "3         b'ENSG00000245105_A2M-AS1'\n",
       "4           b'ENSG00000166535_A2ML1'\n",
       "                    ...             \n",
       "22045        b'ENSG00000198455_ZXDB'\n",
       "22046        b'ENSG00000070476_ZXDC'\n",
       "22047      b'ENSG00000162378_ZYG11B'\n",
       "22048         b'ENSG00000159840_ZYX'\n",
       "22049       b'ENSG00000074755_ZZEF1'\n",
       "Length: 22050, dtype: bytes328"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cite_inputs_cols # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        b'ENSG00000121410'\n",
       "1        b'ENSG00000268895'\n",
       "2        b'ENSG00000175899'\n",
       "3        b'ENSG00000245105'\n",
       "4        b'ENSG00000166535'\n",
       "                ...        \n",
       "23413    b'ENSG00000070476'\n",
       "23414    b'ENSG00000203995'\n",
       "23415    b'ENSG00000162378'\n",
       "23416    b'ENSG00000159840'\n",
       "23417    b'ENSG00000074755'\n",
       "Length: 23418, dtype: bytes120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_multi_targets_cols # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         b'GL000194.1:114519-115365'\n",
       "1           b'GL000194.1:55758-56597'\n",
       "2           b'GL000194.1:58217-58957'\n",
       "3           b'GL000194.1:59535-60431'\n",
       "4         b'GL000195.1:119766-120427'\n",
       "                     ...             \n",
       "228937        b'chrY:7814107-7815018'\n",
       "228938        b'chrY:7818751-7819626'\n",
       "228939        b'chrY:7836768-7837671'\n",
       "228940        b'chrY:7869454-7870371'\n",
       "228941        b'chrY:7873814-7874709'\n",
       "Length: 228942, dtype: bytes208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_multi_inputs_cols # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear model. \n",
    "\n",
    "- Need to do PCA on input because extremely wide\n",
    "- TruncatedSVD time scales with `n_components`, so need to keep sorta small. 1000 was taking forever (nothing after 30min), but 1 components took ~2min. 16 took ~4 min. 32 = 6min. 64 = 9min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components = N_COMPONENTS, random_state=1)\n",
    "reduced_train_multi_inputs_values = pca.fit_transform(train_multi_inputs_values)\n",
    "X = pd.DataFrame(reduced_train_multi_inputs_values, columns = [str(i) for i in range(N_COMPONENTS)])\n",
    "X.to_feather(f'{DATA_DIR}/../reduced/{N_COMPONENTS}_reduced_train_multi_inputs_values.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_hdf('../data/original/train_multi_targets.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Took 16min to train on full data, 64 components\n",
    "# though only ~20seconds to train 10k rows (I blame swapping)\n",
    "model.fit(X, Y)\n",
    "dump(model, '64_reduced_full_multi_LinearRegression.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_multi_targets_values = sp.sparse.load_npz(\n",
    "    f'{SPARSE_DATA_DIR}/test_multi_inputs_values.sparse.npz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TruncatedSVD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb Cella 21\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO: probably want to add this to the training data to do PCA\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# on all available inputs. Might overfit on unseen data, but that's \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# not part of this competition 😈\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pca \u001b[39m=\u001b[39m TruncatedSVD(n_components \u001b[39m=\u001b[39m N_COMPONENTS, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luke/projects/singlecell-kaggle/code/multi-sparse_EDA.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m reduced_test_multi_inputs_values \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TruncatedSVD' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: probably want to add this to the training data to do PCA\n",
    "# on all available inputs. Might overfit on unseen inputs, but that's \n",
    "# not part of this competition 😈\n",
    "pca = TruncatedSVD(n_components = N_COMPONENTS, random_state=1)\n",
    "reduced_test_multi_inputs_values = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(\n",
    "    reduced_test_multi_inputs_values, \n",
    "    columns = [str(i) for i in range(N_COMPONENTS)],\n",
    "    )\n",
    "X_test.to_feather(f'{DATA_DIR}/../reduced/{N_COMPONENTS}_reduced_test_multi_inputs_values.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = load('../data/models/64_reduced_full_multi_LinearRegression.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: below code should be reusable across technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.load(\"../data/sparse/test_multi_inputs_idxcol.npz\",\n",
    "                    allow_pickle=True)[\"index\"]\n",
    "y_columns = np.load(\"../data/sparse/train_multi_targets_idxcol.npz\",\n",
    "                    allow_pickle=True)[\"columns\"]\n",
    "\n",
    "# Maps from row number to cell_id\n",
    "cell_dict = dict((k,i) for i, k in enumerate(test_index))\n",
    "assert( len(cell_dict) == len(test_index))\n",
    "gene_dict = dict((k,i) for i, k in enumerate(y_columns))\n",
    "assert( len(gene_dict) == len(y_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(train_multi_targets_cols) == Y_hat.shape[1])\n",
    "assert(len(test_index) == Y_hat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The below approach was too slow\n",
    "    # Y_hat_indexed = pd.DataFrame(\n",
    "    #     Y_hat, \n",
    "    #     columns = train_multi_targets_cols.values,   # type: ignore\n",
    "    #     index = test_index\n",
    "    # )\n",
    "    # Y_hat_indexed.stack()\n",
    "\n",
    "eval_ids = pd.read_parquet('../data/sparse/evaluation.parquet')\n",
    "\n",
    "# Create two arrays of indices, so that for every row in long `eval_ids` \n",
    "# list we have the coordinates of the corresponding value in the \n",
    "# model's rectangular output matrix. \n",
    "eval_ids_cell_num = eval_ids.cell_id.apply(\n",
    "    lambda x: cell_dict.get(x, -1)\n",
    ")\n",
    "eval_ids_gene_num = eval_ids.gene_id.apply(\n",
    "    lambda x: gene_dict.get(x, -1)\n",
    ")\n",
    "\n",
    "# Eval_id rows that have both and \"x\" and \"y\" index are valid\n",
    "# TODO: should check that nothing has just one (x or y) index\n",
    "valid_multi_rows = (eval_ids_gene_num !=-1) & (eval_ids_cell_num!=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(name='target',\n",
    "                       index=pd.MultiIndex.from_frame(eval_ids), \n",
    "                       dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neat numpy trick to make a 1d array from 2d based on two arrays: \n",
    "# one of \"x\" coordinates and one of \"y\" coordinates of the 2d array.\n",
    "submission.iloc[valid_multi_rows] = Y_hat[  # type: ignore\n",
    "    eval_ids_cell_num[valid_multi_rows].to_numpy(),  # type: ignore\n",
    "    eval_ids_gene_num[valid_multi_rows].to_numpy()  # type: ignore\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_submission = pd.read_csv(\n",
    "    '../data/submissions/cite_linreg.csv',\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[submission.isna()] = cite_submission.head(6812820)['target']\n",
    "submission.reset_index(drop=True, inplace=True)\n",
    "submission.index.name = 'row_id'\n",
    "assert(submission.isna().sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../data/submissions/full_64_reduced_linreg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffd6c8928b0ee1ff35d95e1a002ba83d6fc6c953861d0ac4f834d9a76673592c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
