{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on last year's winning submission:\n",
    "# https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/Guanlab-dengkw/run\n",
    "\n",
    "import logging\n",
    "import utils\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whole notebooks fits in about 1min for 10k rows. \n",
    "- 100k failed with an error: `Canceled future for execute_request message before replies were done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentParameters:\n",
    "    MAX_ROWS = 10_000\n",
    "    TECHNOLOGY: utils.Technology = utils.cite\n",
    "    INPUT_PCA_DIMS = 128\n",
    "    OUTPUT_PCA_DIMS = 128\n",
    "    K_FOLDS = 3\n",
    "    NP_RANDOM_SEED = 1000\n",
    "\n",
    "\n",
    "np.random.seed(ExperimentParameters.NP_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading `h5ad` files...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Reading `h5ad` files...')\n",
    "# TODO: extract to method with params  (sparse/dense, technology)\n",
    "input_train= pd.read_hdf(\n",
    "    ExperimentParameters.TECHNOLOGY.train_inputs_path, \n",
    "    start=0, \n",
    "    stop=ExperimentParameters.MAX_ROWS\n",
    ")\n",
    "output_train= pd.read_hdf(\n",
    "    ExperimentParameters.TECHNOLOGY.train_targets_path,\n",
    "    start=0, \n",
    "    stop=ExperimentParameters.MAX_ROWS\n",
    ")\n",
    "input_test= pd.read_hdf(\n",
    "    ExperimentParameters.TECHNOLOGY.test_inputs_path, \n",
    "    start=0, \n",
    "    stop=ExperimentParameters.MAX_ROWS\n",
    ")\n",
    "\n",
    "pred_dim_x = input_test.shape[0]\n",
    "pred_dim_y = output_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Models using the Truncated SVD to reduce the dimension\n"
     ]
    }
   ],
   "source": [
    "# Do PCA on all input data\n",
    "input_pca_train = pd.concat(\n",
    "    [input_train, input_test],\n",
    "    axis=0,  # type: ignore\n",
    ")\n",
    "\n",
    "logging.info('Models using the Truncated SVD to reduce the dimension')\n",
    "\n",
    "pca_input = TruncatedSVD(n_components=ExperimentParameters.INPUT_PCA_DIMS)\n",
    "# TODO: float16 might be better, saw something in the forum\n",
    "pca_features = pca_input.fit_transform(input_pca_train).astype(np.float32)\n",
    "pca_train_input = pca_features[:len(input_train)] # First len(input_train) rows are input_train\n",
    "pca_test_input = pca_features[len(input_train):] # Last len(input_test) rows are input_test\n",
    "assert( len(pca_train_input) + len(pca_test_input) == len(pca_features))\n",
    "\n",
    "del input_train\n",
    "del input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also do PCA on output (needs to be de-reduced later)\n",
    "pca_output = TruncatedSVD(n_components=ExperimentParameters.OUTPUT_PCA_DIMS)\n",
    "pca_train_output = pca_output.fit_transform(output_train).astype(np.float32)\n",
    "\n",
    "del output_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running row-wise normalization...\n"
     ]
    }
   ],
   "source": [
    "# Last year they found row-wise normalization was helpful, though they had\n",
    "# to deal with more batch effects\n",
    "def row_wise_std_scaler(M):\n",
    "    \"\"\" \n",
    "    Standard scale values by row. \n",
    "    Sklearn StandardScaler has now row-wise option\n",
    "    \"\"\"\n",
    "    std = np.std(M, axis=1).reshape(-1, 1)\n",
    "    # Make any zero std 1 to avoid numerical problems\n",
    "    std[std == 0] = 1\n",
    "    mean = np.mean(M, axis=1).reshape(-1, 1)\n",
    "    return (M - mean) / std\n",
    "\n",
    "logging.info('Running row-wise normalization...')\n",
    "# normalization across gene counts for a single cell.\n",
    "# Possibly useful since we only care about correlation and not magnitude\n",
    "# TODO: do we really want to normalize PCA input?\n",
    "train_norm = row_wise_std_scaler(pca_train_input).astype(np.float32)\n",
    "del pca_train_input \n",
    "test_norm = row_wise_std_scaler(pca_test_input).astype(np.float32)\n",
    "del pca_test_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running KRR model ...\n",
      "INFO:root:Starting k-fold loop...\n",
      "INFO:root:Fitting KRR fold 0 of 3...\n",
      "INFO:root:Score: 0.8908717086652675\n",
      "INFO:root:Fitting KRR fold 1 of 3...\n",
      "INFO:root:Score: 0.8912362154462354\n",
      "INFO:root:Fitting KRR fold 2 of 3...\n",
      "INFO:root:Score: 0.8881603906429922\n"
     ]
    }
   ],
   "source": [
    "logging.info('Running KRR model ...')\n",
    "# TODO: research these more\n",
    "SCALE = 10\n",
    "ALPHA = .2\n",
    "logging.info('Starting k-fold loop...')\n",
    "\n",
    "kf = KFold(n_splits=ExperimentParameters.K_FOLDS)\n",
    "\n",
    "def report_score(score):\n",
    "    logging.info(f\"Score: {score}\")\n",
    "    # TODO: add results writing\n",
    "    \n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(train_norm)):\n",
    "    kernel = RBF(length_scale = SCALE)\n",
    "    krr = KernelRidge(alpha=ALPHA, kernel=kernel)  # type: ignore\n",
    "    logging.info(f'Fitting KRR fold {fold_index} of {ExperimentParameters.K_FOLDS}...')\n",
    "    krr.fit(train_norm[train_index], pca_train_output[train_index])\n",
    "    # TODO: review pca de-reduction\n",
    "    Y_hat = krr.predict(train_norm[test_index]) @ pca_output.components_\n",
    "    Y = pca_train_output[test_index] @ pca_output.components_\n",
    "    score = utils.correlation_score(Y, Y_hat)\n",
    "    report_score(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffd6c8928b0ee1ff35d95e1a002ba83d6fc6c953861d0ac4f834d9a76673592c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
