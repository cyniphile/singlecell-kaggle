{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on last year's winning submission:\n",
    "# https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/Guanlab-dengkw/run\n",
    "\n",
    "import logging\n",
    "import utils\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whole notebooks fits in about 1min for 10k rows. \n",
    "- 100k failed with an error: `Canceled future for execute_request message before replies were done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentParameters:\n",
    "    MAX_ROWS_TRAIN = 10_000\n",
    "    OUTPUT_SUBMISSION = True\n",
    "    TECHNOLOGY = utils.cite\n",
    "    REPO: utils.ExperimentRepository = utils.cite\n",
    "    INPUT_PCA_DIMS = 128\n",
    "    OUTPUT_PCA_DIMS = 128\n",
    "    K_FOLDS = 3\n",
    "    NP_RANDOM_SEED = 1000\n",
    "\n",
    "np.random.seed(ExperimentParameters.NP_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading `h5ad` files...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Reading `h5ad` files...')\n",
    "# TODO: extract to method with params  (sparse/dense, technology)\n",
    "input_train= pd.read_hdf(\n",
    "    ExperimentParameters.TECHNOLOGY.train_inputs_path, \n",
    "    start=0, \n",
    "    stop=ExperimentParameters.MAX_ROWS_TRAIN\n",
    ")\n",
    "output_train= pd.read_hdf(\n",
    "    ExperimentParameters.TECHNOLOGY.train_targets_path,\n",
    "    start=0, \n",
    "    stop=ExperimentParameters.MAX_ROWS_TRAIN\n",
    ")\n",
    "\n",
    "if ExperimentParameters.OUTPUT_SUBMISSION:\n",
    "    input_test= pd.read_hdf(\n",
    "        ExperimentParameters.TECHNOLOGY.test_inputs_path, \n",
    "    )\n",
    "else:\n",
    "    input_test= pd.read_hdf(\n",
    "        ExperimentParameters.TECHNOLOGY.test_inputs_path, \n",
    "        start=0, \n",
    "        stop=ExperimentParameters.MAX_ROWS_TRAIN\n",
    "    )\n",
    "    \n",
    "\n",
    "pred_dim_x = input_test.shape[0]\n",
    "pred_dim_y = output_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Models using the Truncated SVD to reduce the dimension\n"
     ]
    }
   ],
   "source": [
    "# Do PCA on all input data\n",
    "input_pca_train = pd.concat(\n",
    "    [input_train, input_test],\n",
    "    axis=0,  # type: ignore\n",
    ")\n",
    "\n",
    "logging.info('Models using the Truncated SVD to reduce the dimension')\n",
    "\n",
    "pca_input = TruncatedSVD(n_components=ExperimentParameters.INPUT_PCA_DIMS)\n",
    "# TODO: float16 might be better, saw something in the forum\n",
    "pca_features = pca_input.fit_transform(input_pca_train).astype(np.float32)\n",
    "pca_train_input = pca_features[:len(input_train)] # First len(input_train) rows are input_train\n",
    "pca_test_input = pca_features[len(input_train):] # Last len(input_test) rows are input_test\n",
    "assert( len(pca_train_input) + len(pca_test_input) == len(pca_features))\n",
    "\n",
    "del input_train\n",
    "del input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also do PCA on output (needs to be de-reduced later)\n",
    "pca_output = TruncatedSVD(n_components=ExperimentParameters.OUTPUT_PCA_DIMS)\n",
    "pca_train_output = pca_output.fit_transform(output_train).astype(np.float32)\n",
    "\n",
    "del output_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running row-wise normalization...\n"
     ]
    }
   ],
   "source": [
    "# Last year they found row-wise normalization was helpful, though they had\n",
    "# to deal with more batch effects\n",
    "def row_wise_std_scaler(M):\n",
    "    \"\"\" \n",
    "    Standard scale values by row. \n",
    "    Sklearn StandardScaler has now row-wise option\n",
    "    \"\"\"\n",
    "    std = np.std(M, axis=1).reshape(-1, 1)\n",
    "    # Make any zero std 1 to avoid numerical problems\n",
    "    std[std == 0] = 1\n",
    "    mean = np.mean(M, axis=1).reshape(-1, 1)\n",
    "    return (M - mean) / std\n",
    "\n",
    "logging.info('Running row-wise normalization...')\n",
    "# normalization across gene counts for a single cell.\n",
    "# Possibly useful since we only care about correlation and not magnitude\n",
    "# TODO: do we really want to normalize PCA input?\n",
    "train_norm = row_wise_std_scaler(pca_train_input).astype(np.float32)\n",
    "del pca_train_input \n",
    "test_norm = row_wise_std_scaler(pca_test_input).astype(np.float32)\n",
    "del pca_test_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Score:\n",
    "    score: float\n",
    "\n",
    "@dataclass\n",
    "class ScoreSummary:\n",
    "    scores: List[Score]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running KRR model ...\n",
      "INFO:root:Starting k-fold loop...\n",
      "INFO:root:Fitting KRR fold 0 of 3...\n",
      "INFO:root:Fitting KRR fold 1 of 3...\n",
      "INFO:root:Fitting KRR fold 2 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScoreSummary(scores=[Score(score=0.8919446084606477), Score(score=0.8924710202317526), Score(score=0.8887646403539322)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info('Running KRR model ...')\n",
    "# TODO: research these more\n",
    "SCALE = 10\n",
    "ALPHA = .2\n",
    "logging.info('Starting k-fold loop...')\n",
    "\n",
    "kf = KFold(n_splits=ExperimentParameters.K_FOLDS)\n",
    "\n",
    "def report_score_summary(score):\n",
    "    logging.info(f\"Score: {score}\")\n",
    "    # TODO: add results writing\n",
    "    \n",
    "kernel = RBF(length_scale = SCALE)\n",
    "krr = KernelRidge(alpha=ALPHA, kernel=kernel)  # type: ignore\n",
    "\n",
    "# TODO: require clean history (git), write outputs to file\n",
    "def fit_and_score(*,\n",
    "    fold_index: int,\n",
    "    train_indices: np.ndarray,\n",
    "    test_indices: np.ndarray,\n",
    "    pca_output,\n",
    "    ) -> Score:\n",
    "    \"\"\"\n",
    "    performs fit and returns score\n",
    "    \"\"\"\n",
    "    logging.info(f'Fitting KRR fold {fold_index} of {ExperimentParameters.K_FOLDS}...')\n",
    "    krr.fit(train_norm[train_indices], pca_train_output[train_indices])\n",
    "    # TODO: review pca de-reduction\n",
    "    Y_hat = krr.predict(train_norm[test_indices]) @ pca_output.components_\n",
    "    Y = pca_train_output[test_indices] @ pca_output.components_\n",
    "    score = utils.correlation_score(Y, Y_hat)\n",
    "    return Score(score=score)\n",
    "\n",
    "scores = []\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(train_norm)):\n",
    "    score = fit_and_score(\n",
    "        fold_index=fold_index, \n",
    "        train_indices=train_index,\n",
    "        test_indices=test_index,\n",
    "        pca_output = pca_output,\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "ScoreSummary(scores=scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading indices...\n",
      "INFO:root:Loading evaluation ids...\n",
      "INFO:root:Final step: fill empty submission df...\n"
     ]
    }
   ],
   "source": [
    "krr.fit(train_norm, pca_train_output)\n",
    "Y_hat = krr.predict(test_norm) @ pca_output.components_\n",
    "submission_cite = utils.format_submission(Y_hat, utils.cite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old submission which includes multiome tech predictions\n",
    "multi_submission = pd.read_csv(\n",
    "    utils.OUTPUT_DIR / \"full_64_reduced_linreg.csv\",\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop multi-index to align with other submission\n",
    "reindexed_submission_cite = pd.DataFrame(submission_cite.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_valid_submission(submission: pd.DataFrame):\n",
    "    assert submission.index.name == 'row_id'\n",
    "    assert submission.columns == ['target']\n",
    "    assert len(submission) == 65744180\n",
    "    assert submission['target'].isna().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into dataframe with proper column names\n",
    "merged = reindexed_submission_cite['target'].fillna(multi_submission[reindexed_submission_cite['target'].isna()]['target'])\n",
    "formatted_submission = pd.DataFrame(merged, columns=['target'])\n",
    "formatted_submission.index.name = \"row_id\"\n",
    "test_valid_submission(formatted_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write full predictions to csv\n",
    "formatted_submission.to_csv(utils.OUTPUT_DIR / \"cite_rbf_with_multi_linear.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffd6c8928b0ee1ff35d95e1a002ba83d6fc6c953861d0ac4f834d9a76673592c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
